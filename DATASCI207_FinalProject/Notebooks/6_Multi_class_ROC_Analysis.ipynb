{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "EsegRIKw06aN"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate Results with Multiclass ROC Analysis\n",
        "\n",
        "**Goal of this analysis:** Understand our best model's ability to distinguish between the different tumor classes, how the model handles some class imbalance, and generate visuals for threshold selection and comparison.\n",
        "\n",
        "\n",
        "**1) Prepare Data:** Convert the multiclass labels into a binary format for analysis.\n",
        "\n",
        "**2) Compute ROC and AUC:** Calculate ROC curves and AUC scores for each class using a One versus Rest strategy (OvR)\n",
        "\n",
        "**3) Average AUC:** Compute btoh micro-averaged and macro-averaged AUC scores to evaluate overall model performance.\n",
        "\n",
        "**4) Plotting Curves:** Plot the ROC curves for each class and the averaged ROC curves to visualize the model's performance."
      ],
      "metadata": {
        "id": "6Bwht31fjmU9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries"
      ],
      "metadata": {
        "id": "ggHmjzI7jX_V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YoqUX5pTjD5T"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc, RocCurveDisplay\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from itertools import cycle"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load in Test Data"
      ],
      "metadata": {
        "id": "uj-VfT_pjYp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = np.load('X_test.npy')\n",
        "Y_test = np.load('Y_test.npy')"
      ],
      "metadata": {
        "id": "_zmKzezvjUL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One-versus-Rest Classification"
      ],
      "metadata": {
        "id": "7BSUQb4VjZIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert y-test to binary format for computation\n",
        "n_classes = Y_test.max() + 1\n",
        "Y_test_bin = label_binarize(Y_test, classes=[0, 1, 2, 3])"
      ],
      "metadata": {
        "id": "Y79_Gu1JjVbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial Attempt"
      ],
      "metadata": {
        "id": "EsegRIKw06aN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate ROC Curves and AUC Scores"
      ],
      "metadata": {
        "id": "_OICW17yjaK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load hybrid model (best performance)\n",
        "best_model = tf.keras.models.load_model(\"hybrid_transformer_cnn_model.keras\")\n",
        "\n",
        "# assign object for probability predictions\n",
        "Y_score = best_model.predict(X_test)\n",
        "\n",
        "# for each class, calculate roc and auc\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "for i in range(n_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(Y_test_bin[:, i], Y_score[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# calc micro-average\n",
        "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(Y_test_bin.ravel(), Y_score.ravel())\n",
        "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "\n",
        "# calc macro-average\n",
        "\n",
        "# aggregate false positive (FP) rates\n",
        "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
        "\n",
        "# interpolate all ROC curves at these points\n",
        "mean_tpr = np.zeros_like(all_fpr)\n",
        "for i in range(n_classes):\n",
        "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
        "\n",
        "# average it and compute auc\n",
        "mean_tpr /= n_classes\n",
        "\n",
        "fpr[\"macro\"] = all_fpr\n",
        "tpr[\"macro\"] = mean_tpr\n",
        "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n"
      ],
      "metadata": {
        "id": "UCuaMDyxjZ0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizations"
      ],
      "metadata": {
        "id": "Khx4qJerjd1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot ROC curves\n",
        "plt.figure()\n",
        "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
        "         label='micro-average ROC curve (area = {0:0.2f})'\n",
        "               ''.format(roc_auc[\"micro\"]),\n",
        "         color='deeppink', linestyle=':', linewidth=4)\n",
        "\n",
        "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
        "         label='macro-average ROC curve (area = {0:0.2f})'\n",
        "               ''.format(roc_auc[\"macro\"]),\n",
        "         color='navy', linestyle=':', linewidth=4)\n",
        "\n",
        "colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green'])\n",
        "for i, color in zip(range(n_classes), colors):\n",
        "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
        "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
        "                   ''.format(i, roc_auc[i]))\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curves')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "CAQdBa2wjeOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modular approach to run evaluation on all three models (baseline, cnn, hybrid)"
      ],
      "metadata": {
        "id": "XItoI7Eh0LT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, X_test, Y_test, class_names):\n",
        "\n",
        "    # Predict probabilities\n",
        "    Y_test_pred = model.predict(X_test)\n",
        "\n",
        "    # compute ROC AUC for each tumor class\n",
        "    fpr, tpr, aucs = {}, {}, {}\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        fpr[i], tpr[i], _ = roc_curve(Y_test == i, Y_test_pred[:, i])\n",
        "        aucs[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "    # Compute micro and macro average AUC\n",
        "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(Y_test.ravel(), Y_test_pred.ravel())\n",
        "    aucs[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "\n",
        "    # plot ROC curves\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    plt.plot(fpr[\"micro\"], tpr[\"micro\"], label=f'Micro-average ROC curve (area = {aucs[\"micro\"]:.2f})')\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        plt.plot(fpr[i], tpr[i], label=f'ROC curve of class {class_name} (area = {aucs[i]:.2f})')\n",
        "\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "    # generate confusion matrix\n",
        "    Y_test_pred_classes = np.argmax(Y_test_pred, axis=1)\n",
        "    conf_matrix = confusion_matrix(Y_test, Y_test_pred_classes)\n",
        "\n",
        "    # plot confusion matrix\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Greens',\n",
        "                xticklabels=class_names,\n",
        "                yticklabels=class_names)\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "    # gen classification report for analysis\n",
        "    report = classification_report(Y_test, Y_test_pred_classes, target_names=class_names)\n",
        "    print(\"Classification Report:\\n\", report)\n"
      ],
      "metadata": {
        "id": "ho1IXxFn0Kmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set class names\n",
        "class_names = ['No Tumor', 'Meningioma', 'Glioma', 'Pituitary']\n",
        "\n",
        "# run evaluation function on each model\n",
        "print(\"Evaluating Baseline Model:\")\n",
        "evaluate_model(baseline_model, X_test, Y_test, class_names)\n",
        "\n",
        "print(\"\\nEvaluating 2-Layer CNN Model:\")\n",
        "evaluate_model(model_2_layers, X_test, Y_test, class_names)\n",
        "\n",
        "print(\"\\nEvaluating 3-Layer CNN Model:\")\n",
        "evaluate_model(model_3_layers, X_test, Y_test, class_names)\n",
        "\n",
        "print(\"\\nEvaluating Hybrid Model:\")\n",
        "evaluate_model(hybrid_transformer_cnn_model, X_test, Y_test, class_names)"
      ],
      "metadata": {
        "id": "j-HLsmCv1FRS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}